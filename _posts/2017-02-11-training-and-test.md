---
layout: post
title: 机器学习基石之训练与测试
date: 2017-2-11
categories: blog
tags: [机器学习基石与技法]
description: 机器学习基石与技法
---

**回顾：学习的可行性？**         
最重要的是公式：       
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/foundation/chapter5/p1.jpg)       

（1） 假设空间H有限（M），且训练数据足够大，则可以保证测试错误率Eout 约等于训练错误率Ein；            
（2）如果能得到Ein 接近于零，根据（1），Eout 趋向于零。              
以上两条保证的学习的可能性。                                       

可知，假设空间H 的大小M 至关重要，我们接下来会力求找一个H 大小的上界。            
M存在重要的trade-off 思想：       
（1）当M 很小，那么坏数据出现的概率非常小（见第四讲分析），学习是有效的；但是由于假设空间过小，我们不一定能找到一个方案，可以使训练误差接近零；

（2）反之，若M 很大，坏数据出现的概率会变大。            
若假设空间无限大（比如感知机），学习一定是无效的吗？这门在下面力图回答这个问题。           

 假设空间H 大小：M                       
根据上面的公式，当M 无限大时是无法有效学习的；主要是我们在计算M 是通过UNION BOUND 的方式，这样得到的上界太宽松了；实际上，由于不同假设下发生坏事情是有很多重叠的，其实我们可以得到比M小得多的上界。    

我们希望将这么多的假设进行分组，根据组数来考虑假设空间大小。          

