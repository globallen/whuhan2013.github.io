---
layout: post
title: 多变量线性回归
date: 2016-11-29
categories: blog
tags: [机器学习]
description: 多变量线性回归
---



目前为止,我们探讨了单变量/特征的回归模型,现在我们对房价模型增加更多的特征, 例如房间数楼层等,构成一个含有多个变量的模型,模型中的特征为(x1,x2,...,xn)

增添更多特征后,我们引入一系列新的注释:    
n 代表特征的数量   
$x^{(i)}$代表第 i 个训练实例,是特征矩阵中的第 i 行,是一个向量(vector)。    
$x^i_j$ 代表特征矩阵中第 i 行的第 j 个特征,也就是第 i 个训练实例的第 j 个特征。

支持多变量的假设 h 表示为:$h(x)= \theta_0+\theta_1x_1+\theta_2x_2... \theta_nx_n$

这个公式中有 n+1 个参数和 n 个变量,为了使得公式能够简化一些,引入 x0=1,则公式
转化为:$h(x)=\Theta^TX$

#### 多变量梯度下降      

与单变量线性回归类似,在多变量线性回归中,我们也构建一个代价函数,则这个代价 函数是所有建模误差的平方和,即:    
$$j(\theta_0,\theta_1...\theta_n)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$$      
其中:$h_\theta(x)=\Theta^TX=\theta_0x_0+\theta_1x_1+....\theta_nx_n$     

我们的目标和单变量线性回归问题中一样,是要找出使得代价函数最小的一系列参数。
多变量线性回归的批量梯度下降算法为:

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/p9.png) 


**梯度下降法实践 1-特征缩放**      


在我们面对多维特征问题的时候,我们要保证这些特征都具有相近的尺度,这将帮助梯 度下降算法更快地收敛。
以房价问题为例,假设我们使用两个特征,房屋的尺寸和房间的数量,尺寸的值为 0- 2000 平方英尺,而房间数量的值则是 0-5,以两个参数分别为横纵坐标,绘制代价函数的等 高线图能,看出图像会显得很扁,梯度下降算法需要非常多次的迭代才能收敛。


解决的方法是尝试将所有特征的尺度都尽量缩放到-1 到 1 之间。    

$$x_n=\frac{x_n-u_n}{s_n}$$       
其中$u_n$表示平均值，$s_n$表示标准差       

**梯度下降法实践 2-学习率**      
梯度下降算法收敛所需要的迭代次数根据模型的不同而不同,我们不能提前预知,我们 可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/p10.png) 

也有一些自动测试是否收敛的方法,例如将代价函数的变化值与某个阀值(例如 0.001) 进行比较,但通常看上面这样的图表更好。
梯度下降算法的每次迭代受到学习率的影响,如果学习率 α 过小,则达到收敛所需的迭 代次数会非常高;如果学习率 α 过大,每次迭代可能不会减小代价函数,可能会越过局部最 小值导致无法收敛。

通常可以考虑尝试些学习率: α=0.01,0.03,0.1,0.3,1,3,10
 
 
**特征和多项式回归**       

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/p11.png)       

注:如果我们采用多项式回归模型,在运行梯度下降算法前,特征缩放非常有必要。

 