---
layout: post
title: 神经网络结构与神经元激励函数
date: 2017-3-2
categories: blog
tags: [计算机视觉]
description: 计算机视觉
---


**单个神经元建模**

神经网络算法领域最初是被对生物神经系统建模这一目标启发，但随后与其分道扬镳，成为一个工程问题，并在机器学习领域取得良好效果。我们可以这么理解这个模型：在信号的传导过程中，突触可以控制传导到下一个神经元的信号强弱(数学模型中的权重w)，而这种强弱是可以学习到的。在我们简化的数学计算模型中，我们假定有一个『激励函数』来控制加和的结果对神经元的刺激程度，从而控制着是否激活神经元和向后传导信号

#### 作为线性分类器的单个神经元

神经元模型的前向计算数学公式看起来可能比较眼熟。就像在线性分类器中看到的那样，神经元有能力“喜欢”（激活函数值接近1），或者不喜欢（激活函数值接近0）输入空间中的某些线性区域。因此，只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter5/p1.png)

**对于正则化的解释**           
对于正则化的损失函数(不管是SVM还是Softmax)，其实我们在神经元的生物特性上都能找到对应的解释，我们可以将其(正则化项的作用)视作信号在神经元传递过程中的逐步淡化/衰减(gradual forgetting)，因为正则化项的作用是在每次迭代过程中，控制住权重w的幅度，往0上靠拢。

单个神经元的作用，可视作完成一个二分类的分类器(比如Softmax或者SVM分类器)

#### 常用激励函数

每一次输入和权重w线性组合之后，都会通过一个激励函数(也可以叫做非线性激励函数)，经非线性变换后输出。实际的神经网络中有一些可选的激励函数，我们一一说明一下最常见的几种：

**sigmoid**          
<center><img src="https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter5/p2.jpeg"></center>

sigmoid函数提到的次数太多，相信大家都知道了。数学形式很简单，是$σ(x)=1/(1+e^{−x})$，图像如上图所示，功能是把一个实数压缩至0到1之间。输入的数字非常大的时候，结果会接近1，而非常大的负数作为输入，则会得到接近0的结果。不得不说，早期的神经网络中，sigmoid函数作为激励函数使用非常之多，因为大家觉得它很好地解释了神经元受到刺激后是否被激活和向后传递的场景(从几乎没有被激活，也就是0，到完全被激活，也就是1)。不过似乎近几年的实际应用场景中，比较少见到它的身影，它主要的缺点有2个：

- sigmoid函数在实际梯度下降中，容易饱和和终止梯度传递。我们来解释一下，大家知道反向传播过程，依赖于计算的梯度，在一元函数中，即斜率。而在sigmoid函数图像上，大家可以很明显看到，在纵坐标接近0和1的那些位置(也就是输入信号的幅度很大的时候)，斜率都趋于0了。我们回想一下反向传播的过程，我们最后用于迭代的梯度，是由中间这些梯度值结果相乘得到的，因此如果中间的局部梯度值非常小，直接会把最终梯度结果拉近0，也就是说，残差回传的过程，因为sigmoid函数的饱和被杀死了。说个极端的情况，如果一开始初始化权重的时候，我们取值不是很恰当，而激励函数又全用的sigmoid函数，那么很有可能神经元一个不剩地饱和到无法学习，整个神经网络也根本没办法训练起来。
- sigmoid函数的输出没有0中心化，这是一个比较闹心的事情，因为每一层的输出都要作为下一层的输入，而未0中心化会直接影响梯度下降，我们这么举个例子吧，如果输出的结果均值不为0，举个极端的例子，全部为正的话(例如$f=w^Tx+b$中所有x>0)，那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式f而定），这带来的后果是，梯度更新的时候，不是平缓地迭代变化，而是类似锯齿状的突变。当然，要多说一句的是，这个缺点相对于第一个缺点，还稍微好一点，当把整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题，第一个缺点的后果是，很多场景下，神经网络根本没办法学习。      


**Tanh**           
<center><img src="https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter5/p6.jpeg"></center>
Tanh函数的图像如上图所示。它会将输入值压缩至-1到1之间，当然，它同样也有sigmoid函数里说到的第一个缺点，在很大或者很小的输入值下，神经元很容易饱和。但是它缓解了第二个缺点，它的输出是0中心化的。所以在实际应用中，tanh激励函数还是比sigmoid要用的多一些的。
$tanh=2\sigma(2x)-1$

**ReLU**           
<center><img src="https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter5/p7.jpeg"></center>
ReLU是修正线性单元(The Rectified Linear Unit)的简称，近些年使用的非常多，图像如上图所示。它对于输入x计算f(x)=max(0,x)。换言之，以0为分界线，左侧都为0，右侧是y=x这条直线。        
它有它对应的优势，也有缺点：                   

优点1：实验表明，它的使用，相对于sigmoid和tanh，可以非常大程度地提升随机梯度下降的收敛速度。不过有意思的是，很多人说，这个结果的原因是它是线性的，而不像sigmoid和tanh一样是非线性的。具体的收敛速度结果对比如下图，收敛速度大概能快上6倍：            
<center><img src="https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter5/p8.jpeg"></center>




