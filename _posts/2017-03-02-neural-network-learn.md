---
layout: post
title: 神经网络结构与神经元激励函数
date: 2017-3-2
categories: blog
tags: [计算机视觉]
description: 计算机视觉
---


**单个神经元建模**

神经网络算法领域最初是被对生物神经系统建模这一目标启发，但随后与其分道扬镳，成为一个工程问题，并在机器学习领域取得良好效果。我们可以这么理解这个模型：在信号的传导过程中，突触可以控制传导到下一个神经元的信号强弱(数学模型中的权重w)，而这种强弱是可以学习到的。在我们简化的数学计算模型中，我们假定有一个『激励函数』来控制加和的结果对神经元的刺激程度，从而控制着是否激活神经元和向后传导信号

#### 作为线性分类器的单个神经元

神经元模型的前向计算数学公式看起来可能比较眼熟。就像在线性分类器中看到的那样，神经元有能力“喜欢”（激活函数值接近1），或者不喜欢（激活函数值接近0）输入空间中的某些线性区域。因此，只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter5/p1.png)

**对于正则化的解释**           
对于正则化的损失函数(不管是SVM还是Softmax)，其实我们在神经元的生物特性上都能找到对应的解释，我们可以将其(正则化项的作用)视作信号在神经元传递过程中的逐步淡化/衰减(gradual forgetting)，因为正则化项的作用是在每次迭代过程中，控制住权重w的幅度，往0上靠拢。

单个神经元的作用，可视作完成一个二分类的分类器(比如Softmax或者SVM分类器)

#### 常用激励函数

每一次输入和权重w线性组合之后，都会通过一个激励函数(也可以叫做非线性激励函数)，经非线性变换后输出。实际的神经网络中有一些可选的激励函数，我们一一说明一下最常见的几种：

**sigmoid**          
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter5/p2.jpeg)

sigmoid函数提到的次数太多，相信大家都知道了。数学形式很简单，是$σ(x)=1/(1+e^{−x})$，图像如上图所示，功能是把一个实数压缩至0到1之间。输入的数字非常大的时候，结果会接近1，而非常大的负数作为输入，则会得到接近0的结果。不得不说，早期的神经网络中，sigmoid函数作为激励函数使用非常之多，因为大家觉得它很好地解释了神经元受到刺激后是否被激活和向后传递的场景(从几乎没有被激活，也就是0，到完全被激活，也就是1)。不过似乎近几年的实际应用场景中，比较少见到它的身影，它主要的缺点有2个：


- sigmoid函数在实际梯度下降中，容易饱和和终止梯度传递。我们来解释一下，大家知道反向传播过程，依赖于计算的梯度，在一元函数中，即斜率。而在sigmoid函数图像上，大家可以很明显看到，在纵坐标接近0和1的那些位置(也就是输入信号的幅度很大的时候)，斜率都趋于0了。我们回想一下反向传播的过程，我们最后用于迭代的梯度，是由中间这些梯度值结果相乘得到的，因此如果中间的局部梯度值非常小，直接会把最终梯度结果拉近0，也就是说，残差回传的过程，因为sigmoid函数的饱和被杀死了。说个极端的情况，如果一开始初始化权重的时候，我们取值不是很恰当，而激励函数又全用的sigmoid函数，那么很有可能神经元一个不剩地饱和到无法学习，整个神经网络也根本没办法训练起来。

