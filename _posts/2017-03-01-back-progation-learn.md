---
layout: post
title: 计算机视觉之反向传播 
date: 2017-3-1
categories: blog
tags: [计算机视觉]
description: 计算机视觉
---


**问题描述与动机：**     

- 大家都知道的，其实我们就是在给定的图像像素向量x和对应的函数f(x)，然后我们希望能够计算f在x上的梯度(∇f(x))
- 我们之所以想解决这个问题，是因为在神经网络中，f对应损失函数L，而输入x则对应训练样本数据和神经网络的权重W。举一个特例，损失函数可以是SVM loss function，而输入则对应样本数据(xi,yi),i=1…N和权重以及bias W,b。需要注意的一点是，在我们的场景下，通常我们认为训练数据是给定的，而权重是我们可以控制的变量。因此我们为了更新权重的等参数，使得损失函数值最小，我们通常是计算f对参数W,b的梯度。不过我们计算其在xi上的梯度有时候也是有用的，比如如果我们想做可视化以及了解神经网络在『做什么』的时候。

**高数梯度/偏导基础**         
每个维度/变量上的偏导，表示整个函数表达式，在这个值上的『敏感度』

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter4/p1.png)

```
x = -2; y = 5; z = -4

# 前向计算
q = x + y # q becomes 3
f = q * z # f becomes -12

# 类反向传播:
# 先算到了 f = q * z
dfdz = q # df/dz = q
dfdq = z # df/dq = z
# 再算到了 q = x + y
dfdx = 1.0 * dfdq # dq/dx = 1 恩，链式法则
dfdy = 1.0 * dfdq # dq/dy = 1
```

链式法则的结果是，只剩下我们感兴趣的[dfdx,dfdy,dfdz]，也就是原函数在x,y,z上的偏导。这是一个简单的例子，之后的程序里面我们为了简洁，不会完整写出dfdq，而是用dq代替。

以下是这个计算的示意图： 
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter4/p2.jpeg)    

**反向传播的直观理解**      

一句话概括：反向传播的过程，实际上是一个由局部到全部的精妙过程。比如上面的电路图中，其实每一个『门』在拿到输入之后，都能计算2个东西：

- 输出值
- 对应输入和输出的局部梯度

而且很明显，每个门在进行这个计算的时候是完全独立的，不需要对电路图中其他的结构有了解。然而，在整个前向传输过程结束之后，在反向传播过程中，每个门却能逐步累积计算出它在整个电路输出上的梯度。『链式法则』告诉我们每一个门接收到后向传来的梯度，同时用它乘以自己算出的对每个输入的局部梯度，接着往后传。

以上面的图为例，来解释一下这个过程。加法门接收到输入[-2, 5]同时输出结果3。因为加法操作对两个输入的偏导都应该是1。电路后续的部分算出最终结果-12。在反向传播过程中，链式法则是这样做的：加法操作的输出3，在最后的乘法操作中，获得的梯度为-4，如果把整个网络拟人化，我们可以认为这代表着网络『想要』加法操作的结果小一点，而且是以4*的强度来减小。加法操作的门获得这个梯度-4以后，把它分别乘以本地的两个梯度(加法的偏导都是1)，1*-4=-4。如果输入x减小，那加法门的输出也会减小，这样乘法输出会相应的增加。

反向传播，可以看做网络中门与门之间的『关联对话』，它们『想要』自己的输出更大还是更小(以多大的幅度)，从而让最后的输出结果更大。
