---
layout: post
title: 神经网络数据预处理，正则化与损失函数
date: 2017-3-5
categories: blog
tags: [计算机视觉]
description: 计算机视觉
---


本文主要包括以下内容          

- 设置数据和模型
	+ 数据预处理
	+ 权重初始化
	+ 批量归一化（Batch Normalization）
	+ 正则化（L2/L1/Maxnorm/Dropout）
- 损失函数
- 小结

#### 设置数据和模型    
在上一节中介绍了神经元的模型，它在计算内积后进行非线性激活函数计算，神经网络将这些神经元组织成各个层。这些做法共同定义了评分函数（score function）的新形式，该形式是从前面线性分类章节中的简单线性映射发展而来的。具体来说，神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算。本节将讨论更多的算法设计选项，比如数据预处理，权重初始化和损失函数。

#### 数据预处理
在卷积神经网处理图像问题的时候，图像数据有3种常见的预处理可能会用到，如下。我们假定数据表示成矩阵为X，其中我们假定X是[N*D]维矩阵(N是样本数据量，D为单张图片的数据向量长度)。             

- 去均值，这是最常见的图片数据预处理，简单说来，它做的事情就是，对待训练的每一张图片的特征，都减去全部训练集图片的特征均值，这么做的直观意义就是，我们把输入数据各个维度的数据都中心化到0了。使用python的numpy工具包，这一步可以用X -= np.mean(X, axis = 0)轻松实现。当然，其实这里也有不同的做法：简单一点，我们可以直接求出所有像素的均值，然后每个像素点都减掉这个相同的值；稍微优化一下，我们在RGB三个颜色通道分别做这件事。
- 归一化，归一化的直观理解含义是，我们做一些工作去保证所有的维度上数据都在一个变化幅度上。通常我们有两种方法来实现归一化。一个是在数据都去均值之后，每个维度上的数据都除以这个维度上数据的标准差(X /= np.std(X, axis = 0))。另外一种方式是我们除以数据绝对值最大值，以保证所有的数据归一化后都在-1到1之间。多说一句，其实在任何你觉得各维度幅度变化非常大的数据集上，你都可以考虑归一化处理。不过对于图像而言，其实这一步反倒可做可不做，因为大家都知道，像素的值变化区间都在[0,255]之间，所以其实图像输入数据天生幅度就是一致的。

上述两个操作对于数据的作用，画成示意图，如下：
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter6/p1.jpeg)

PCA和白化/whitening，这是另外一种形式的数据预处理。在经过去均值操作之后，我们可以计算数据的协方差矩阵，从而可以知道数据各个维度之间的相关性，简单示例代码如下：

```
# 假定输入数据矩阵X是[N*D]维的
X -= np.mean(X, axis = 0) # 去均值
cov = np.dot(X.T, X) / X.shape[0] # 计算协方差
```

数据协方差矩阵的第(i, j)个元素是数据第i个和第j个维度的协方差。具体来说，该矩阵的对角线上的元素是方差。还有，协方差矩阵是对称和半正定的。我们可以对数据协方差矩阵进行SVD（奇异值分解）运算。

```
U,S,V = np.linalg.svd(cov)
```

U的列是特征向量，S是装有奇异值的1维数组（因为cov是对称且半正定的，所以S中元素是特征值的平方）。为了去除数据相关性，将已经零中心化处理过的原始数据投影到特征基准上：

```
Xrot = np.dot(X,U) # 对数据去相关性
```

这么理解一下可能更好，U是一组正交基向量。所以我们可以看做把原始数据X投射到这组维度保持不变的正交基底上，从而也就完成了对原始数据的去相关。如果去相关之后你再求一下Xrot的协方差矩阵，你会发现这时候的协方差矩阵是一个对角矩阵了。而numpy中的np.linalg.svd更好的一个特性是，它返回的U是对特征值排序过的，这也就意味着，我们可以用它进行降维操作。我们可以只取top的一些特征向量，然后做和原始数据做矩阵乘法，这个时候既降维减少了计算量，同时又保存下了绝大多数的原始数据信息，这就是所谓的主成分分析/PCA：

```
Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced 变成 [N x 100]
```


这个操作之后，我们把原始数据集矩阵从[N*D]降维到[N*100]，保存了前100个能包含绝大多数数据信息的维度。实际应用中，你在PCA降维之后的数据集上，做各种机器学习的训练，在节省空间和时间的前提下，依旧能有很好的训练准确度。


最后一个在实践中会看见的变换是**白化**（whitening）。白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。该操作的代码如下：

```
# 对数据进行白化操作:
# 除以特征值 
Xwhite = Xrot / np.sqrt(S + 1e-5)
```

提个醒：whitening操作会有严重化噪声的可能。注意到我们在上述代码中，分母的部分加入了一个很小的数1e-5，以防止出现除以0的情况。但是数据中的噪声部分可能会因whitening操作而变大，因为这个操作的本质是把输入的每个维度都拉到差不多的幅度，那么本不相关的有微弱幅度变化的噪声维度，也被拉到了和其他维度同样的幅度。当然，我们适当提高的安全因子(1e-5)可以在一定程度上缓解这个问题。

下图为原始数据到去相关到白化之后的数据分布示意图： 
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter6/p2.jpeg)

我们来看看真实数据集上的操作与得到的结果，也许能对这些过程有更清晰一些的认识。大家都还记得CIFAR-10图像数据集吧。训练集大小为50000*3072，也就是说，每张图片都被展成一个3072维度的列向量了。然后我们对原始50000*3072数据矩阵做SVD分解，进行上述一些操作，再可视化一下，得到的结果示意图如下：
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter6/p3.jpeg)

最左：一个用于演示的集合，含49张图片。左二：3072个特征值向量中的前144个。靠前面的特征向量解释了数据中大部分的方差，可以看见它们与图像中较低的频率相关。第三张是49张经过了PCA降维处理的图片，展示了144个特征向量。这就是说，展示原始图像是每个图像用3072维的向量，向量中的元素是图片上某个位置的像素在某个颜色通道中的亮度值。而现在每张图片只使用了一个144维的向量，其中每个元素表示了特征向量对于组成这张图片的贡献度。为了让图片能够正常显示，需要将144维度重新变成基于像素基准的3072个数值。因为U是一个旋转，可以通过乘以U.transpose()[:144,:]来实现，然后将得到的3072个数值可视化。可以看见图像变得有点模糊了，这正好说明前面的特征向量获取了较低的频率。然而，大多数信息还是保留了下来。最右：将“白化”后的数据进行显示。其中144个维度中的方差都被压缩到了相同的数值范围。然后144个白化后的数值通过乘以U.transpose()[:144,:]转换到图像像素基准上。现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了。

实际工程中，因为这个部分讲到数据预处理，我们就把基本的几种数据预处理都讲了一遍，但实际卷积神经网中，我们并没有用到去相关和whitening操作。当然，去均值是非常非常重要的，而每个像素维度的归一化也是常用的操作。

**常见错误**       
进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。应该怎么做呢？应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。

#### 权重初始化
我们之前已经看过一个完整的神经网络，是怎么样通过神经元和连接搭建起来的，以及如何对数据做预处理。在训练神经网络之前，我们还有一个任务要做，那就是初始化参数。

**错误的想法：全部初始化为0**              
有些同学说，那既然要训练和收敛嘛，初始值就随便设定，简单一点就全设为0好了。亲，这样是绝对不行的！！！为啥呢？我们在神经网络训练完成之前，是不可能预知神经网络最后的权重具体结果的，但是根据我们归一化后的数据，我们可以假定，大概有半数左右的权重是正数，而另外的半数是负数。但设定全部初始权重都为0的结果是，网络中每个神经元都计算出一样的结果，然后在反向传播中有一样的梯度结果，因此迭代之后的变化情况也都一样，这意味着这个神经网络的权重没有办法差异化，也就没有办法学习到东西。

**很小的随机数**          
其实我们依旧希望初始的权重是较小的数，趋于0，但是就像我们刚刚讨论过的一样，不要真的是0。综合上述想法，在实际场景中，我们通常会把初始权重设定为非常小的数字，然后正负尽量一半一半。这样，初始的时候权重都是不一样的很小随机数，然后迭代过程中不会再出现迭代一致的情况。举个例子，我们可能可以这样初始化一个权重矩阵W=0.0001*np.random.randn(D,H)。这个初始化的过程，使得每个神经元的权重向量初始化为多维高斯中的随机采样向量，所以神经元的初始权重值指向空间中的随机方向。


特别说明：其实不一定更小的初始值会比大值有更好的效果。我们这么想，一个有着非常小的权重的神经网络在后向传播过程中，回传的梯度也是非常小的。这样回传的”信号”流会相对也较弱，对于层数非常多的深度神经网络，这也是一个问题，回传到最前的迭代梯度已经很小了。

**方差归一化**          
上面提到的建议有一个小问题，对于随机初始化的神经元参数下的输出，其分布的方差随着输入的数量，会增长。我们实际上可以通过除以总输入数目的平方根，归一化每个神经元的输出方差到1。也就是说，我们倾向于初始化神经元的权重向量为w = np.random.randn(n) / sqrt(n)，其中n为输入数。
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter6/p5.png)         

对于初始化权重还有一些类似的研究和建议，比如说Glorot在论文Understanding the difficulty of training deep feedforward neural networks就推荐使用能满足$Var(w)=2/(n_{in}+n_{out})$的权重初始化。其中$n_{in},n_{out}$是前一层和后一层的神经元个数。而另外一篇比较新的论文Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification，则指出尤其对于ReLU神经元，我们初始化方差应该为2.0/n，也就是w = np.random.randn(n) * sqrt(2.0/n)，目前的神经网络中使用了很多ReLU单元，因此这个设定其实在实际应用中使用最多。


**偏移量/bias初始化**          相对而言，bias项初始化就简单一些。我们很多时候简单起见，直接就把它们都设为0.在ReLU单元中，有些同学会使用很小的数字(比如0.01)来代替0作为所有bias项的初始值，他们解释说这样也能保证ReLU单元一开始就是被激活的，因此反向传播过程中不会终止掉回传的梯度。不过似乎实际的实验过程中，这个优化并不是每次都能起到作用的，因此很多时候我们还是直接把bias项都初始化为0。    

**实践**         
当前的推荐是使用ReLU激活函数，并且使用w = np.random.randn(n) * sqrt(2.0/n)来进行权重初始化

**批量归一化（Batch Normalization）**        
[批量归一化](https://arxiv.org/abs/1502.03167)是loffe和Szegedy最近才提出的方法，该方法减轻了如何合理初始化神经网络这个棘手问题带来的头痛：），其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。因为归一化是一个简单可求导的操作，所以上述思路是可行的。在实现层面，应用这个技巧通常意味着全连接层（或者是卷积层，后续会讲）与激活函数之间添加一个BatchNorm层。对于这个技巧本节不会展开讲，因为上面的参考文献中已经讲得很清楚了，需要知道的是在神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。最后一句话总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。搞定！



#### 正则化
在前一节里我们说了我们要通过正则化来控制神经网络，使得它不那么容易过拟合。有几种正则化的类型供选择：   

- L2正则化，这个我们之前就提到过，非常常见。实现起来也很简单，我们在损失函数里，加入对每个参数的惩罚度。也就是说，对于每个权重w，我们在损失函数里加入一项$0.5λw^2$，其中λ是我们可调整的正则化强度。顺便说一句，这里在前面加上1/2的原因是，求导/梯度的时候，刚好变成λw而不是2λw。L2正则化理解起来也很简单，它对于特别大的权重有很高的惩罚度，以求让权重的分配均匀一些，而不是集中在某一小部分的维度上。我们再想想，加入L2正则化项，其实意味着，在梯度下降参数更新的时候，每个权重以W += -lambda*W的程度被拉向0。   

- L1正则化，这也是一种很常见的正则化形式。在L1正则化中，我们对于每个权重w的惩罚项为λw。有时候，你甚至可以看到大神们混着L1和L2正则化用，也就是说加入惩罚项$λ_1∣w∣+λ_2w^2$，L1正则化有其独特的特性，它会让模型训练过程中，权重特征向量逐渐地稀疏化，这意味着到最后，我们只留下了对结果影响最大的一部分权重，而其他不相关的输入(例如『噪声』)因为得不到权重被抑制。所以通常L2正则化后的特征向量是一组很分散的小值，而L1正则化只留下影响较大的权重。在实际应用中，如果你不是特别要求只保留部分特征，那么L2正则化通常能得到比L1正则化更好的效果        

- 最大范数约束，另外一种正则化叫做最大范数约束，它直接限制了一个上行的权重边界，然后约束每个神经元上的权重都要满足这个约束。实际应用中是这样实现的，我们不添加任何的惩罚项，就按照正常的损失函数计算，只不过在得到每个神经元的权重向量$w⃗$之后约束它满足$∥w⃗ ∥_2<c$。有些人提到这种正则化方式帮助他们提高最后的模型效果。另外，这种正则化方式倒是有一点很吸引人：在神经网络训练学习率设定很高的时候，它也能很好地约束住权重更新变化，不至于直接挂掉。                   

- Dropout，亲，这个是我们实际神经网络训练中，用的非常多的一种正则化手段，同时也相当有效。Srivastava等人的论文Dropout: A Simple Way to Prevent Neural Networks from Overfitting最早提到用dropout这种方式作为正则化手段。一句话概括它，就是：在训练过程中，我们对每个神经元，都以概率p保持它是激活状态，1-p的概率直接关闭它。在训练的时候，随机失活的实现方法是让神经元以超参数p的概率被激活或者被设置为0。        

下图是一个3层的神经网络的dropout示意图：               
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter6/p6.jpeg)

可以这么理解，在训练过程中呢，我们对全体神经元，以概率p做了一个采样，只有选出的神经元要进行参数更新。所以最后就从左图的全连接到右图的Dropout过后神经元连接图了。需要多说一句的是，在测试阶段，我们不用dropout，而是直接从概率的角度，对权重配以一个概率值。

简单的Dropout代码如下(这是简易实现版本，但是不建议使用，我们会分析为啥，并在之后给出优化版)：

```
""" 普通版随机失活: 不推荐实现 (看下面笔记) """

p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱

def train_step(X):
  """ X中是输入数据 """
  
  # 3层neural network的前向传播
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = np.random.rand(*H1.shape) < p # 第一个随机失活遮罩
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = np.random.rand(*H2.shape) < p # 第二个随机失活遮罩
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3
  
  # 反向传播:计算梯度... (略)
  # 进行参数更新... (略)
  
def predict(X):
  # 前向传播时模型集成
  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # 注意：激活数据要乘以p
  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # 注意：激活数据要乘以p
  out = np.dot(W3, H2) + b3
```

上述代码中，在train_step函数中，我们做了2次Dropout。我们甚至可以在输入层做一次dropout。反向传播过程保持不变，除了我们要考虑一下U1,U2

很重要的一点是，大家仔细看predict函数部分，我们不再dropout了，而是对于每个隐层的输出，都用概率p做了一个幅度变换。可以从数学期望的角度去理解这个做法，我们考虑一个神经元的输出为x(没有dropout的情况下)，它的输出的数学期望为px+(1−p)0，那我们在测试阶段，如果直接把每个输出x都做变换x→px，其实是可以保持一样的数学期望的。

上述代码的写法有一些缺陷，我们必须在测试阶段对每个神经的输出都以p的概率输出。考虑到实际应用中，测试阶段对于时间的要求非常高，我们可以考虑反着来，代码实现的时候用inverted dropout，即在训练阶段就做相反的幅度变换/scaling(除以p)，这样在测试阶段，我们可以直接把权重拿来使用，而不用附加很多步用p做scaling的过程。inverted dropout的示例代码如下：


```
""" 
Inverted Dropout的版本，把本该花在测试阶段的时间，转移到训练阶段，从而提高testing部分的速度
"""

p = 0.5 # dropout的概率，也就是保持一个神经元激活状态的概率

def train_step(X):
  # f3层神经网络前向计算
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(*H1.shape) < p) / p # 注意到这个dropout中我们除以p，做了一个inverted dropout
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(*H2.shape) < p) / p # 这个dropout中我们除以p，做了一个inverted dropout
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3

  # 反向传播: 计算梯度... (这里省略)
  # 参数更新... (这里省略)

def predict(X):
  # 直接前向计算，无需再乘以p
  H1 = np.maximum(0, np.dot(W1, X) + b1) 
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3
```

对于dropout这个部分如果你有更深的兴趣，欢迎阅读以下文献：            
1) 2014 Srivastava 的论文Dropout paper          
2) Dropout Training as Adaptive Regularization            

- bias项的正则化，我们大部分时候并不对偏移量项做正则化，因为它们也没有和数据直接有乘法等交互，也就自然不会影响到最后结果中某个数据维度的作用。不过如果你愿意对它做正则化，倒也不会影响最后结果，毕竟总共有那么多权重项，才那么些bias项，所以一般也不会影响结果。


实际应用中：我们最常见到的是，在全部的交叉验证集上使用L2正则化，同时我们在每一层之后用dropout，很常见的dropout概率为p=0.5，你也可以通过交叉验证去调整这个值。

#### 损失函数
刚才讨论了数据预处理、权重初始化与正则化相关的问题。现在我们回到训练需要的关键之一：损失函数。对于这么复杂的神经网络，我们也得有一个评估准则去评估预测值和真实结果之间的吻合度，也就是损失函数。神经网络里的损失函数，实际上是计算出了每个样本上的loss，再求平均之后的一个形式，即$L=1/N∑_iL_i$，其中N是训练样本数。

**分类问题**         
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter6/p7.png)  

问题：类别数目巨大。当标签集非常庞大（例如字典中的所有英语单词，或者ImageNet中的22000种分类），就需要使用分层Softmax（Hierarchical Softmax）了（参考文献）。分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。

