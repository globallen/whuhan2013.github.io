---
layout: post
title: 大规模机器学习
date: 2016-12-18
categories: blog
tags: [机器学习]
description: 大规模机器学习
---

如果我们有一个低方差的模型,增加数据集的规模可以帮助你获得更好的结果。我们应 该怎样应对一个有 100 万条记录的训练集?        
以线性回归模型为例,每一次梯度下降迭代,我们都需要计算训练集的误差的平方和, 如果我们的学习算法需要有 20 次迭代,这便已经是非常大的计算代价。    
首先应该做的事是去检查一个这么大规模的训练集是否真的必要,也许我们只用 1000 个训练集也能获得较好的效果,我们可以绘制学习曲线来帮助判断。  

**随机梯度下降法**      
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class12/p1.png)  


随机梯度下降算法在每一次计算之后便更新参数 θ,而不需要首先将所有的训练集求和, 在梯度下降算法还没有完成一次迭代时,随机梯度下降算法便已经走出了很远。但是这样的 算法存在的问题是,不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全 局最小值的位置,但是可能无法站到那个最小值的那一点,而是在最小值点附近徘徊。

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class12/p2.png)  

**小批量梯度下降**      
小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法,每计算 常数 b 次训练实例,便更新一次参数 θ。      

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class12/p3.png)  
通常我们会令 b 在 2-100 之间。这样做的好处在于,我们可以用向量化的方式来循环 b 个训练实例,如果我们用的线性代数函数库比较好,能够支持平行处理,那么算法的总体 表现将不受影响(与随机梯度下降相同)。

**随机梯度下降收敛**     
现在我们介绍随机梯度下降算法的调试,以及学习率 α 的选取。       

在批量梯度下降中,我们可以令代价函数 J 为迭代次数的函数,绘制图表,根据图表来 判断梯度下降是否收敛。但是,在大规模的训练集的情况下,这是不现实的,因为计算代价 太大了。

在随机梯度下降中,我们在每一次更新 θ 之前都计算一次代价,然后每 X 次迭代后,求 出这 X 次对训练实例计算代价的平均值,然后绘制这些平均值与 X 次迭代的次数之间的函 数图表。
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class12/p4.png)  

当我们绘制这样的图表时,可能会得到一个颠簸不平但是不会明显减少的函数图像(如 上面左下图中蓝线所示)。我们可以增加 X 来使得函数更加平缓,也许便能看出下降的趋势 了(如上面左下图中红线所示);或者可能函数图表仍然是颠簸不平且不下降的(如洋红色 线所示),那么我们的模型本身可能存在一些错误。

如果我们得到的曲线如上面右下方所示,不断地上升,那么我们可能会需要选择一个较 小的学习率 α。 

我们也可以令学习率随着迭代次数的增加而减小,例如令:    
$$a=\frac{const1}{interationNumber+const2}$$      

随着我们不断地靠近全局最小值,通过减小学习率,我们迫使算法收敛而非在最小值附 近徘徊。 但是通常我们不需要这样做便能有非常好的效果了,对 α 进行调整所耗费的计算 通常不值得     
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class12/p5.png)  

总结下,这段视频中,我们介绍了一种方法,近似地监测出随机梯度下降算法在最优化 代价函数中的表现,这种方法不需要定时地扫 整个训练集,来算出整个样本集的代价函数, 而是只需要每次对最后 1000 个,或者多少个样本,求一下平均值。应用这种方法,你既可 以保证随机梯度下降法正在正常运转和收敛,也可以用它来调整学习速率α的大小。

