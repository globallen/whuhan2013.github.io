---
layout: post
title: 神经网络训练细节与注意点
date: 2017-3-7
categories: blog
tags: [计算机视觉]
description: 计算机视觉
---


本文主要包括以下内容：       

- 梯度检查
- 合理性（Sanity）检查
- 检查学习过程
	- 损失函数
	- 训练集与验证集准确率
	- 权重：更新比例
	- 每层的激活数据与梯度分布
	- 可视化 
- 参数更新
	- 一阶（随机梯度下降）方法，动量方法，Nesterov动量方法
	- 学习率退火
	- 二阶方法
	- 逐参数适应学习率方法（Adagrad，RMSProp）
- 超参数调优
- 评价
	- 模型集成
- 总结
- 拓展引用


#### 梯度检查

理论上将进行梯度检查很简单，就是简单地把解析梯度和数值计算梯度进行比较。然而从实际操作层面上来说，这个过程更加复杂且容易出错。下面是一些提示、技巧和需要仔细注意的事情：

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter7/p1.png)       
加max项的原因很简单：整体形式变得简单和对称。再提个小醒，别忘了避开分母中两项都为0的情况。OK，对于相对误差而言            

- 相对误差>1e-2意味着你的实现肯定是有问题的
- 1e-2>相对误差>1e-4，你会有点担心
- 1e-4>相对误差，基本是OK的，但是要注意极端情况(使用tanh或者softmax时候出现kinks)那还是太大
- 1e-7>相对误差，放心大胆使用

**使用双精度浮点数**         
如果你使用单精度浮点数计算，那你的实现可能一点问题都没有，但是相对误差却很大。实际工程中出现过，从单精度切到双精度，相对误差立马从1e-2降到1e-8的情况。

**要留意浮点数的范围**           
一篇很好的文章是What Every Computer Scientist Should Know About Floating-Point Arithmetic。我们得保证计算时，所有的数都在浮点数的可计算范围内，太小的值(通常绝对值小于1e-10就绝对让人担心)会带来计算上的问题。如果确实过小，可以使用一个常数暂时将损失函数的数值范围扩展到一个更“好”的范围，在这个范围中浮点数变得更加致密。比较理想的是1.0的数量级上，即当浮点数指数为0时。

**目标函数的不可导点（kinks）**                         
在进行梯度检查时，一个导致不准确的原因是不可导点问题。不可导点是指目标函数不可导的部分,它指的是一种会导致数值梯度和解析梯度不一致的情况。会出现在使用ReLU或者类似的神经单元上时，对于很小的负数，比如x=-1e-6，因为x<0，所以解析梯度是绝对为0的，但是对于数值梯度而言，加入你计算f(x+h)，取的h>1e-6，那就跳到大于0的部分了，这样数值梯度就一定和解析梯度不一样了。而且这个并不是极端情况哦，对于一个像CIFAR-10这样级别的数据集，因为有50000个样本，会有450000个max(0,x)，会出现很多的kinks。

不过我们可以监控max里的2项，比较大的那项如果存在跃过0的情况，那就要注意了。           

**使用少量数据点**           
解决上面的不可导点问题的一个办法是使用更少的数据点。因为含有不可导点的损失函数(例如：因为使用了ReLU或者边缘损失等函数)的数据点越少，不可导点就越少，所以在计算有限差值近似时越过不可导点的几率就越小。还有，如果你的梯度检查对2-3个数据点都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用很少量的数据点，能让梯度检查更迅速高效。



