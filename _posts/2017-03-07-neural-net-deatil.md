---
layout: post
title: 神经网络训练细节与注意点
date: 2017-3-7
categories: blog
tags: [计算机视觉]
description: 计算机视觉
---


本文主要包括以下内容：       

- 梯度检查
- 合理性（Sanity）检查
- 检查学习过程
	- 损失函数
	- 训练集与验证集准确率
	- 权重：更新比例
	- 每层的激活数据与梯度分布
	- 可视化 
- 参数更新
	- 一阶（随机梯度下降）方法，动量方法，Nesterov动量方法
	- 学习率退火
	- 二阶方法
	- 逐参数适应学习率方法（Adagrad，RMSProp）
- 超参数调优
- 评价
	- 模型集成
- 总结
- 拓展引用


#### 梯度检查

理论上将进行梯度检查很简单，就是简单地把解析梯度和数值计算梯度进行比较。然而从实际操作层面上来说，这个过程更加复杂且容易出错。下面是一些提示、技巧和需要仔细注意的事情：

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter7/p1.png)       
加max项的原因很简单：整体形式变得简单和对称。再提个小醒，别忘了避开分母中两项都为0的情况。OK，对于相对误差而言            

- 相对误差>1e-2意味着你的实现肯定是有问题的
- 1e-2>相对误差>1e-4，你会有点担心
- 1e-4>相对误差，基本是OK的，但是要注意极端情况(使用tanh或者softmax时候出现kinks)那还是太大
- 1e-7>相对误差，放心大胆使用

**使用双精度浮点数**         
如果你使用单精度浮点数计算，那你的实现可能一点问题都没有，但是相对误差却很大。实际工程中出现过，从单精度切到双精度，相对误差立马从1e-2降到1e-8的情况。

**要留意浮点数的范围**           
一篇很好的文章是What Every Computer Scientist Should Know About Floating-Point Arithmetic。我们得保证计算时，所有的数都在浮点数的可计算范围内，太小的值(通常绝对值小于1e-10就绝对让人担心)会带来计算上的问题。如果确实过小，可以使用一个常数暂时将损失函数的数值范围扩展到一个更“好”的范围，在这个范围中浮点数变得更加致密。比较理想的是1.0的数量级上，即当浮点数指数为0时。

**目标函数的不可导点（kinks）**                         
在进行梯度检查时，一个导致不准确的原因是不可导点问题。不可导点是指目标函数不可导的部分,它指的是一种会导致数值梯度和解析梯度不一致的情况。会出现在使用ReLU或者类似的神经单元上时，对于很小的负数，比如x=-1e-6，因为x<0，所以解析梯度是绝对为0的，但是对于数值梯度而言，加入你计算f(x+h)，取的h>1e-6，那就跳到大于0的部分了，这样数值梯度就一定和解析梯度不一样了。而且这个并不是极端情况哦，对于一个像CIFAR-10这样级别的数据集，因为有50000个样本，会有450000个max(0,x)，会出现很多的kinks。

不过我们可以监控max里的2项，比较大的那项如果存在跃过0的情况，那就要注意了。           

**使用少量数据点**           
解决上面的不可导点问题的一个办法是使用更少的数据点。因为含有不可导点的损失函数(例如：因为使用了ReLU或者边缘损失等函数)的数据点越少，不可导点就越少，所以在计算有限差值近似时越过不可导点的几率就越小。还有，如果你的梯度检查对2-3个数据点都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用很少量的数据点，能让梯度检查更迅速高效。

**设定步长h要小心**          
h肯定不能特别大，这个大家都知道对吧。但我并不是说h要设定的非常小，其实h设定的非常小也会有问题，因为h太小程序可能会有精度问题。很有意思的是，有时候在实际情况中h如果从非常小调为1e-4或者1e-6反倒会突然计算变得正常。


**不要让正则化项盖过数据项**         
通常损失函数是数据损失和正则化损失的和（例如L2对权重的惩罚）。需要注意的危险是正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分（正则化部分的梯度表达式通常简单很多）。这样就会掩盖掉数据损失梯度的不正确实现。因此，推荐先关掉正则化对数据损失做单独检查，然后对正则化做单独检查。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。


**记得关闭随机失活（dropout）和数据扩张（augmentation）**       
在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。不然它们会在计算数值梯度的时候导致巨大误差。关闭这些操作不好的一点是无法对它们进行梯度检查（例如随机失活的反向传播实现可能有错误）。因此，一个更好的解决方案就是在计算f(x+h)和f(x-h)前强制增加一个特定的随机种子，在计算解析梯度时也同样如此。


**检查少量的维度**           
在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度然后假设其他维度是正确的。注意：确认在所有不同的参数实际情况中，梯度可能有上百万维参数。因此每个维度都检查一遍就不太现实了，一般都是只检查一些维度，然后假定其他的维度也都正确。要小心一点：要保证这些维度的每个参数都检查对比过了。

#### 训练前的检查工作              
在开始训练之前，我们还得做一些检查，来确保不会运行了好一阵子，才发现计算代价这么大的训练其实并不正确。        

- 在初始化之后看一眼loss。其实我们在用很小的随机数初始化神经网络后，第一遍计算loss可以做一次检查(当然要记得把正则化系数设为0)。以CIFAR-10为例，如果使用Softmax分类器，我们预测应该可以拿到值为2.302左右的初始loss(因为10个类别，初始概率应该都为0.1，Softmax损失是-log(正确类别的概率):-ln(0.1)=2.302)。对于Weston Watkins SVM，假设所有的边界都被越过（因为所有的分值都近似为零），所以损失值是9（因为对于每个错误分类，边界值是1）。如果没看到这些损失值，那么初始化中就可能有问题。            

- 加回正则项，接着我们把正则化系数设为正常的小值，加回正则化项，这时候再算损失/loss，应该比刚才要大一些。

- 试着去拟合一个小的数据集。最后一步，也是很重要的一步，在对大数据集做训练之前，我们可以先训练一个小的数据集(比如20张图片)，然后看看你的神经网络能够做到0损失/loss(当然，是指的正则化系数为0的情况下)，因为如果神经网络实现是正确的，在无正则化项的情况下，完全能够过拟合这一小部分的数据。但是注意，能对小数据集进行过拟合并不代表万事大吉，依然有可能存在不正确的实现。比如，因为某些错误，数据点的特征是随机的，这样算法也可能对小数据进行过拟合，但是在整个数据集上跑算法的时候，就没有任何泛化能力。


#### 训练过程中的监控
开始训练之后，我们可以通过监控一些指标来了解训练的状态。我们还记得有一些参数是我们认为敲定的，比如学习率，比如正则化系数。

**损失/loss随每轮完整迭代后的变化**                
下面这幅图表明了不同的学习率下，我们每轮完整迭代(这里的一轮完整迭代指的是所有的样本都被过了一遍，因为随机梯度下降中batch size的大小设定可能不同，因此我们不选每次mini-batch迭代为周期)过后的loss应该呈现的变化状况： 
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter7/p2.jpeg)   
合适的学习率可以保证每轮完整训练之后，loss都减小，且能在一段时间后降到一个较小的程度。太小的学习率下loss减小的速度很慢，如果太激进，设置太高的学习率，开始的loss减小速度非常可观，可是到了某个程度之后就不再下降了，在离最低点一段距离的地方反复，无法下降了。下图是实际训练CIFAR-10的时候，loss的变化情况： 
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter7/p3.jpeg)   
大家可能会注意到上图的曲线有一些上下跳动，不稳定，这和随机梯度下降时候设定的batch size有关系。batch size非常小的情况下，会出现很大程度的不稳定，如果batch size设定大一些，会相对稳定一点。

**训练集/验证集上的准确度**           
然后我们需要跟踪一下训练集和验证集上的准确度状况，以判断分类器所处的状态(过拟合程度如何)：            
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter7/p4.jpeg)







