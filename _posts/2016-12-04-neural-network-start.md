---
layout: post
title: 神经网络入门
date: 2016-12-4
categories: blog
tags: [机器学习]
description: 神经网络入门
---

我们之前学的,无论是线性回归还是逻辑回归都有这样一个缺点,即:当特征太多时, 计算的负荷会非常大。      
普通的逻辑回归模型,不能有效地处理这么多的特征,这时候我们 需要神经网络。     

**神经元和大脑**      
神经网络是一种很古老的算法,它最初产生的目的是制造能模拟大脑的机器。      
神经网络逐渐兴起于二十世纪八九十年代,应用得非常广泛。但由于各种原因,在 90 年代的后期应用减少了。但是最近,神经网络又东山再起了。其中一个原因是:神经网络是 计算量有些偏大的算法。然而大概由于近些年计算机的运行速度变快,才足以真正运行起大 规模的神经网络.      

我们能学习数学,学着做微积分,而且大脑能处理各种不同的令人惊奇的事情。似乎如 果你想要模仿它,你得写很多不同的软件来模拟所有这些五花八门的奇妙的事情。不过能不 能假设大脑做所有这些,不同事情的方法,不需要用上千个不同的程序去实现。相反的,大 脑处理的方法,只需要一个单一的学习算法就可以了?

通过一系列的实验,被 称为神经重接实验,从这个意义上说,如果人体有同一块脑组织可以处理光、声或触觉信号, 那么也许存在一种学习算法,可以同时处理视觉、听觉和触觉,而不是需要运行上千个不同 的程序,或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要 做的就是找出一些近似的或实际的大脑学习算法,然后实现它大脑通过自学掌握如何处理这 些不同类型的数据。在很大的程度上,可以猜想如果我们把几乎任何一种传感器接入到大脑 的几乎任何一个部位的话,大脑就会学会处理它。

#### 模型表示      

为了构建神经网络模型,我们需要首先思考大脑中的神经网络是怎样的?每一个神经元 都可以被认为是一个处理单元/神经核(processing unit/ Nucleus),它含有许多输入/树突 (input/Dendrite),并且有一个输出/轴突(output/Axon)。神经网络是大量神经元相互链 接并通过电脉冲来交流的一个网络。

神经网络模型建立在很多神经元之上,每一个神经元又是一个个学习模型。这些神经元 (也叫激活单元,activation unit)采纳一些特征作为输出,并且根据本身的模型 供一个输 出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例,在神经网络中,参数又可 被成为权重(weight)。

我们设计出了类似于神经元的神经网络,效果如下:   

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p1.png)

其中 x1,x2,x3 是输入单元(input units),我们将原始数据输入给它们。
a1,a2,a3 是中间单元,它们负责将数据进行处理,然后呈递到下一层。 最后是输出单元,它负责计算 hθ(x)。 神经网络模型是许多逻辑单元按照不同层级组织起来的网络,每一层的输出变量都是下
一层的输入变量。下图为一个 3 层的神经网络,第一层成为输入层(Input Layer),最后一 层称为输出层(Output Layer),中间一层成为隐藏层(Hidden Layers)。我们为每一层都增 加一个偏差单位(bias unit):


![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p2.png)

对于上图所示的模型,激活单元和输出分别表达为:       

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p3.png)

上面进行的讨论中只是将特征矩阵中的一行(一个训练实例)喂给了神经网络,我们需 要将整个训练集都喂给我们的神经网络算法来学习模型。
我们可以知道:每一个 a 都是由上一层所有的 x 和每一个 x 所对应的决定的。 (我们把这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION ))

( FORWARD PROPAGATION ) 相对与使用循环来编码,利用向量化的方法会使得计算更 为简便。以上面的神经网络为例,试着计算第二层的值:

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p4.png)

这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算, 我们需要将训练集特征矩阵进行转置,使得同一个实例的特征都在同一列里。即:      
$z^{(2)}=\Theta^{(1)}*X^T$     
$a^{(2)}=g(z^{(2)})$     

为了更好了了解 Neuron Networks 的工作原理,我们先把左半部分遮住:    

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p5.png)

我们可以把 a0,a1,a2,a3 看成更为高级的特征值,也就是 x0,x1,x2,x3 的进化体,并且它们是 由 x 与决定的,因为是梯度下降的,所以 a 是变化的,并且变得越来越厉害,所以这些更高 级的特征值远比仅仅将 x 次方厉害,也能更好的预测新数据。      
  这就是神经网络相比于逻辑回归和线性回归的优势。

