---
layout: post
title: 神经网络入门
date: 2016-12-4
categories: blog
tags: [机器学习]
description: 神经网络入门
---

我们之前学的,无论是线性回归还是逻辑回归都有这样一个缺点,即:当特征太多时, 计算的负荷会非常大。      
普通的逻辑回归模型,不能有效地处理这么多的特征,这时候我们 需要神经网络。     

**神经元和大脑**      
神经网络是一种很古老的算法,它最初产生的目的是制造能模拟大脑的机器。      
神经网络逐渐兴起于二十世纪八九十年代,应用得非常广泛。但由于各种原因,在 90 年代的后期应用减少了。但是最近,神经网络又东山再起了。其中一个原因是:神经网络是 计算量有些偏大的算法。然而大概由于近些年计算机的运行速度变快,才足以真正运行起大 规模的神经网络.      

我们能学习数学,学着做微积分,而且大脑能处理各种不同的令人惊奇的事情。似乎如 果你想要模仿它,你得写很多不同的软件来模拟所有这些五花八门的奇妙的事情。不过能不 能假设大脑做所有这些,不同事情的方法,不需要用上千个不同的程序去实现。相反的,大 脑处理的方法,只需要一个单一的学习算法就可以了?

通过一系列的实验,被 称为神经重接实验,从这个意义上说,如果人体有同一块脑组织可以处理光、声或触觉信号, 那么也许存在一种学习算法,可以同时处理视觉、听觉和触觉,而不是需要运行上千个不同 的程序,或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要 做的就是找出一些近似的或实际的大脑学习算法,然后实现它大脑通过自学掌握如何处理这 些不同类型的数据。在很大的程度上,可以猜想如果我们把几乎任何一种传感器接入到大脑 的几乎任何一个部位的话,大脑就会学会处理它。

#### 模型表示      

为了构建神经网络模型,我们需要首先思考大脑中的神经网络是怎样的?每一个神经元 都可以被认为是一个处理单元/神经核(processing unit/ Nucleus),它含有许多输入/树突 (input/Dendrite),并且有一个输出/轴突(output/Axon)。神经网络是大量神经元相互链 接并通过电脉冲来交流的一个网络。

神经网络模型建立在很多神经元之上,每一个神经元又是一个个学习模型。这些神经元 (也叫激活单元,activation unit)采纳一些特征作为输出,并且根据本身的模型 供一个输 出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例,在神经网络中,参数又可 被成为权重(weight)。

我们设计出了类似于神经元的神经网络,效果如下:   

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p1.png)

其中 x1,x2,x3 是输入单元(input units),我们将原始数据输入给它们。
a1,a2,a3 是中间单元,它们负责将数据进行处理,然后呈递到下一层。 最后是输出单元,它负责计算 hθ(x)。 神经网络模型是许多逻辑单元按照不同层级组织起来的网络,每一层的输出变量都是下
一层的输入变量。下图为一个 3 层的神经网络,第一层成为输入层(Input Layer),最后一 层称为输出层(Output Layer),中间一层成为隐藏层(Hidden Layers)。我们为每一层都增 加一个偏差单位(bias unit):


![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p2.png)

对于上图所示的模型,激活单元和输出分别表达为:       

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p3.png)

上面进行的讨论中只是将特征矩阵中的一行(一个训练实例)喂给了神经网络,我们需 要将整个训练集都喂给我们的神经网络算法来学习模型。
我们可以知道:每一个 a 都是由上一层所有的 x 和每一个 x 所对应的决定的。 (我们把这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION ))

( FORWARD PROPAGATION ) 相对与使用循环来编码,利用向量化的方法会使得计算更 为简便。以上面的神经网络为例,试着计算第二层的值:

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p4.png)

这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算, 我们需要将训练集特征矩阵进行转置,使得同一个实例的特征都在同一列里。即:      
$z^{(2)}=\Theta^{(1)}*X^T$     
$a^{(2)}=g(z^{(2)})$     

为了更好了了解 Neuron Networks 的工作原理,我们先把左半部分遮住:    

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p5.png)

我们可以把 a0,a1,a2,a3 看成更为高级的特征值,也就是 x0,x1,x2,x3 的进化体,并且它们是 由 x 与决定的,因为是梯度下降的,所以 a 是变化的,并且变得越来越厉害,所以这些更高 级的特征值远比仅仅将 x 次方厉害,也能更好的预测新数据。      
  这就是神经网络相比于逻辑回归和线性回归的优势。

#### 特征和直观理解      
从本质上讲,神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中, 我们被限制为使用数据中的原始特征 x1,x2,...,xn,我们虽然可以使用一些二项式项来组合这 些特征,但是我们仍然受到这些原始特征的限制。在神经网络中,原始特征只是输入层,在 我们上面三层的神经网络例子中,第三层也就是输出层做出的预测利用的是第二层的特征, 而非输入层中的原始特征,我们可以认为第二层中的特征是神经网络通过学习后自己得出的 一系列用于预测输出变量的新特征。     

神经网络中,单层神经元(无中间层)的计算可用来表示逻辑运算,比如逻辑 AND、逻 辑或OR 。
举例说明:逻辑与 AND;下图中左半部分是神经网络的设计与 output 层表达式,右边 上部分是 sigmod 函数,下半部分是真值表。

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p6.png)


二元逻辑运算符(BINARY LOGICAL OPERATORS)当输入特征为布尔值(0 或 1)时,我 们可以用一个单一的激活层可以作为二元逻辑运算符,为了表示不同的运算符,我们之需要 选择不同的权重即可。   

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p7.png)    

我们可以利用神经元来组合成更为复杂的神经网络以实现更复杂的运算。例如我们要实 现 XNOR 功能(输入的两个值必须一样,均为 1 或均为 0),即 XNOR=(x1ANDx2) OR((NOTx1)AND(NOTx2))
首先构造一个能表达(NOTx1)AND(NOTx2)部分的神经元:

然后将表示 AND 的神经元和表示(NOTx1)AND(NOTx2)的神经元以及表示 OR 的神经元 进行组合:

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/machineLearning/class4/p8.png)

我们就得到了一个能实现 XNOR 运算符功能的神经网络。 按这种方法我们可以逐渐构造出越来越复杂的函数,也能得到更加厉害的特征值。 这就是神经网络的厉害之处。


