---
layout: post
title: 卷积神经网络
date: 2017-3-14
categories: blog
tags: [计算机视觉]
description: 计算机视觉
---

本文内容如下：  

- 结构概述
- 用来构建卷积神经网络的各种层
  + 卷积层
  + 汇聚层
  + 归一化层
  + 全连接层
  + 将全连接层转化成卷积层
- 卷积神经网络的结构
  + 层的排列规律
  + 层的尺寸设置规律
  + 案例学习（LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet）
  + 计算上的考量
- 拓展资源

**卷积神经网络（CNNs / ConvNets）**        
卷积神经网络是由神经元组成，神经元中有具有学习能力的权重和偏差。每个神经元都得到一些输入数据，进行内积运算后再进行激活函数运算。整个网络依旧是一个可导的评分函数：该函数的输入是原始的图像像素，输出是不同类别的评分。在最后一层（往往是全连接层），网络依旧有一个损失函数（比如SVM或Softmax），并且在神经网络中我们实现的各种技巧和要点依旧适用于卷积神经网络。

那么有哪些地方变化了呢？卷积神经网络的结构基于一个假设，即输入数据是图像，基于该假设，我们就向结构中添加了一些特有的性质。这些特有属性使得前向传播函数实现起来更高效，并且大幅度降低了网络中参数的数量。


**结构概述**        
我们前面讲过的神经网络结构都比较一致，输入层和输出层中间夹着数层隐藏层，每一层都由多个神经元组成，层和层之间是全连接的结构，同一层的神经元之间没有连接。

卷积神经网络是上述结构的一种特殊化处理，因为对于图像这种数据而言，上面这种结构实际应用起来有较大的困难：就拿CIFAR-10举例吧，图片已经很小了，是32*32*3(长宽各32像素，3个颜色通道)的，那么在神经网络当中，我们只看隐藏层中的一个神经元，就应该有32*32*3=3072个权重，如果大家觉得这个权重个数的量还行的话，再设想一下，当这是一个包含多个神经元的多层神经网(假设n个)，再比如图像的质量好一点(比如是200*200*3的)，那将有200*200*3*n= 120000n个权重需要训练，结果是拉着这么多参数训练，基本跑不动，跑得起来也是『气喘吁吁』，当然，最关键的是这么多参数的情况下，分分钟模型就过拟合了。别急，别急，一会儿我们会提到卷积神经网络的想法和简化之处。

卷积神经网络结构比较固定的原因之一，是图片数据本身的合理结构，类图像结构(200*200*3)，我们也把卷积神经网络的神经元排布成 width*height*depth的结构，也就是说这一层总共有width*height*depth个神经元(这里的深度指的是激活数据体的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数），如下图所示。举个例子说，CIFAR-10中的图像是作为卷积神经网络的输入，该数据体的维度是32x32x3（宽度，高度和深度）,CIFAR-10的输出层就是1*1*10维的。另外我们后面会说到，每一层的神经元，其实只和上一层里某些小区域进行连接，而不是和上一层每个神经元全连接。 

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter10/p1.jpeg)

#### 卷积神经网络的组成层
在卷积神经网络中，有3种最主要的层,一个简单的卷积神经网络是由各种层按照顺序排列组成，网络中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层。

- 卷积运算层
- 汇聚(pooling)层
- 全连接层

一个完整的神经网络就是由这三种层叠加组成的。         

网络结构例子：这仅仅是个概述，下面会更详解的介绍细节。一个用于CIFAR-10图像数据分类的卷积神经网络的结构可以是[输入层-卷积层-ReLU层-汇聚层-全连接层]。细节如下：

- 输入[32x32x3]存有图像的原始像素值，本例中图像宽高均为32，有3个颜色通道。
- 卷积层中，神经元与输入层中的一个局部区域相连，每个神经元都计算自己与输入层相连的小区域与自己权重的内积。卷积层会计算所有神经元的输出。如果我们使用12个滤波器（也叫作核），得到的输出数据体的维度就是[32x32x12]。
- ReLU层将会逐个元素地进行激活函数操作，比如使用以0为阈值的max(0,x)作为激活函数。该层对数据尺寸没有改变，还是[32x32x12]。
- 汇聚层在在空间维度（宽度和高度）上进行降采样（downsampling）操作，数据尺寸变为[16x16x12]。
- 全连接层将会计算分类评分，数据尺寸变为[1x1x10]，其中10个数字对应的就是CIFAR-10中10个类别的分类评分值。正如其名，全连接层与常规神经网络一样，其中每个神经元都与前一层中所有神经元相连接。

由此看来，卷积神经网络一层一层地将图像从原始像素值变换成最终的分类评分值。其中有的层含有参数，有的没有。具体说来，卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数（神经元的突触权值和偏差）。而ReLU层和汇聚层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。

小结：

- 简单案例中卷积神经网络的结构，就是一系列的层将输入数据变换为输出数据（比如分类评分）。
- 卷积神经网络结构中有几种不同类型的层（目前最流行的有卷积层、全连接层、ReLU层和汇聚层）。
- 每个层的输入是3D数据，然后使用一个可导的函数将其变换为3D的输出数据。
- 有的层有参数，有的没有（卷积层和全连接层有，ReLU层和汇聚层没有）。
- 有的层有额外的超参数，有的没有（卷积层、全连接层和汇聚层有，ReLU层没有）。

![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter10/p2.jpg)

#### 卷积层       
卷积层是构建卷积神经网络的核心层，它产生了网络中大部分的计算量         
**卷积层综述**          
直观看来，卷积层的参数其实可以看做，一系列的可训练/学习的过滤器。在前向计算过程中，我们输入一定区域大小(width*height)的数据，和过滤器点乘后等到新的二维数据，然后滑过一个个滤波器，组成新的3维输出数据。而我们可以理解成每个过滤器都只关心过滤数据小平面内的部分特征，当出现它学习到的特征的时候，就会呈现激活/activate态。           

局部关联度。这是卷积神经网络的独特之处其中之一，我们知道在高维数据(比如图片)中，用全连接的神经网络，实际工程中基本是不可行的。卷积神经网络中每一层的神经元只会和上一层的一些局部区域相连，这就是所谓的局部连接性。你可以想象成，上一层的数据区，有一个滑动的窗口，只有这个窗口内的数据会和下一层神经元有关联，当然，这个做法就要求我们手动敲定一个超参数:窗口大小。通常情况下，这个窗口的长和宽是相等的，我们把长x宽叫做receptive field。实际的计算中，这个窗口是会『滑动』的，会近似覆盖图片的所有小区域。

举个实例，CIFAR-10中的图片输入数据为[32*32*3]的，如果我们把receptive field设为5*5，那receptive field的data都会和下一层的神经元关联，所以共有5*5*3=75个权重，注意到最后的3依旧代表着RGB 3个颜色通道。

如果不是输入数据层，中间层的data格式可能是[16*16*20]的，假如我们取3*3的receptive field，那单个神经元的权重为3*3*20=180。
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter10/p3.jpeg)
局部关联细节。我们刚才说到卷积层的局部关联问题，这个地方有一个receptive field，也就是我们直观理解上的『滑动数据窗口』。从输入的数据到输出数据，有三个超参数会决定输出数据的维度，分别是深度/depth，步长/stride 和 填充值/zero-padding：

- 所谓深度/depth，简单说来指的就是卷积层中和上一层同一个输入区域连接的神经元个数,它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。。这部分神经元会在遇到输入中的不同feature时呈现activate状态，举个例子，如果这是第一个卷积层，那输入到它的数据实际上是像素值，不同的神经元可能对图像的边缘。轮廓或者颜色会敏感。
- 所谓步长/stride，是指的窗口从当前位置到下一个位置，『跳过』的中间数据个数。比如从图像数据层输入到卷积层的情况下，也许窗口初始位置在第1个像素，第二个位置在第5个像素，那么stride=5-1=4.
- 所谓zero-padding是在原始数据的周边补上0值的圈数。(下面第2张图中的样子)       

这么解释可能理解起来还是会有困难，我们找两张图来对应一下这三个量：         
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter10/p4.jpeg)
这是解决ImageNet分类问题用到的卷积神经网络的一部分，我们看到卷积层直接和最前面的图像层连接。图像层的维度为[227*227*3]，而receptive field设为11*11，图上未标明，但是滑动窗口的步长stride设为4，深度depth为48+48=96(这是双GPU并行设置)，边缘没有补0，因此zero-padding为0，因此窗口滑完一行，总共停留次数为(data_len-receptive_field_len+2*zero-padding)/stride+1=(227-11+2*0)/4+1=55，因为图像的长宽相等，因此纵向窗口数也是55，最后得到的输出数据维度为55*55*96维。
![](https://raw.githubusercontent.com/whuhan2013/myImage/master/cs231n/chapter10/p5.gif)
这是一张动态的卷积层计算图，图上的zero-padding为1，所以大家可以看到数据左右各补了一行0，窗口的长宽为3，滑动步长stride为2。

关于zero-padding，补0这个操作产生的根本原因是，为了保证窗口的滑动能从头刚好到尾。举个例子说，上2图中的上面一幅图，因为(data_len-receptive_field_len+2*zero-padding)/stride刚好能够整除，所以窗口左侧贴着数据开始位置，滑到尾部刚好窗口右侧能够贴着数据尾部位置，因此是不需要补0的。而在下面那幅图中，如果滑动步长设为4，你会发现第一次计算之后，窗口就无法『滑动』了，而尾部的数据，是没有被窗口『看到过』的，因此补0能够解决这个问题。

关于窗口滑动步长。大家可以发现一点，窗口滑动步长设定越小，两次滑动取得的数据，重叠部分越多，但是窗口停留的次数也会越多，运算律大一些；窗口滑动步长设定越长，两次滑动取得的数据，重叠部分越少，窗口停留次数也越少，运算量小，但是从一定程度上说数据信息不如上面丰富了。

**卷积层的参数共享**         
首先得说卷积层的参数共享是一个非常赞的处理方式，它使得卷积神经网络的训练计算复杂度和参数个数降低非常非常多。就拿实际解决ImageNet分类问题的卷积神经网络结构来说，我们知道输出结果有55*55*96=290400个神经元，而每个神经元因为和窗口内数据的连接，有11*11*3=363个权重和1个偏移量。所以总共有290400*364=105705600个权重。。。然后。。。恩，训练要累挂了。。。

因此我们做了一个大胆的假设，我们刚才提到了，每一个神经元可以看做一个filter，对图片中的数据窗区域做『过滤』。那既然是filter，我们干脆就假设这个神经元用于连接数据窗的权重是固定的，这意味着，对同一个神经元而言，不论上一层数据窗口停留在哪个位置，连接两者之间的权重都是同一组数。那代表着，上面的例子中的卷积层，我们只需要 神经元个数*数据窗口维度=96*11*11*3=34848个权重。




